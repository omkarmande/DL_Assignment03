{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required libraries\nimport os\nimport numpy as np\nfrom tensorflow.keras import layers, models, preprocessing\nfrom tensorflow.keras.optimizers import Adam\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding and Verifying GPU \nimport torch\nimport tensorflow as tf\n\n# Check PyTorch device availability\nprint(\"PyTorch CUDA available:\", torch.cuda.is_available())\ntorch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"PyTorch using device: {torch_device}\")\n\n# Check TensorFlow device availability\nprint(\"\\nTensorFlow GPU devices:\", tf.config.list_physical_devices('GPU'))\nprint(f\"TensorFlow using device: {'GPU' if tf.test.is_gpu_available() else 'CPU'}\")\n\n# Set random seeds for reproducibility\ndef set_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seeds()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preparation\nclass DataLoader:\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.input_tokenizer = None\n        self.target_tokenizer = None\n    \n    def load_dataset(self, filename):\n        \"\"\"Load dataset from TSV file\"\"\"\n        with open(os.path.join(self.data_path, filename), encoding='utf-8') as f:\n            return [line.strip().split('\\t') for line in f if '\\t' in line]\n    \n    def preprocess_data(self, train_file, val_file):\n        \"\"\"Preprocess and tokenize data\"\"\"\n        train_data = self.load_dataset(train_file)\n        val_data = self.load_dataset(val_file)\n        \n        # Prepare texts\n        train_source = [x[1] for x in train_data]\n        train_target = [x[0] for x in train_data]\n        \n        # Create tokenizers\n        self.input_tokenizer = self._create_tokenizer(train_source + [x[1] for x in val_data])\n        self.target_tokenizer = self._create_tokenizer(\n            ['\\t' + t for t in train_target] + \n            [t + '\\n' for t in train_target]\n        )\n        \n        # Convert to sequences\n        max_source_len = max(len(s) for s in train_source)\n        max_target_len = max(len(t) for t in train_target) + 1\n        \n        train_enc = self._text_to_sequence(train_source, self.input_tokenizer, max_source_len)\n        train_dec_in = self._text_to_sequence(['\\t' + t for t in train_target], self.target_tokenizer, max_target_len)\n        train_dec_out = np.expand_dims(\n            self._text_to_sequence([t + '\\n' for t in train_target], self.target_tokenizer, max_target_len),\n            -1\n        )\n        \n        return (train_enc, train_dec_in, train_dec_out), self.target_tokenizer\n    \n    def _create_tokenizer(self, texts):\n        \"\"\"Create character-level tokenizer\"\"\"\n        tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=False)\n        tokenizer.fit_on_texts(texts)\n        return tokenizer\n    \n    def _text_to_sequence(self, texts, tokenizer, max_len):\n        \"\"\"Convert texts to padded sequences\"\"\"\n        seq = tokenizer.texts_to_sequences(texts)\n        return preprocessing.sequence.pad_sequences(seq, padding='post', maxlen=max_len)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Building\ndef build_seq2seq_model(input_vocab_size, target_vocab_size, config):\n    \"\"\"Build seq2seq model with specified architecture\"\"\"\n    \n    # Input layers\n    encoder_input = layers.Input(shape=(None,))\n    decoder_input = layers.Input(shape=(None,))\n    \n    # Shared embedding\n    embedding = layers.Embedding(input_vocab_size, config.embedding_dim, mask_zero=True)\n    \n    # Encoder\n    encoder_output, encoder_states = build_encoder(\n        encoder_input, embedding, config)\n    \n    # Decoder\n    decoder_output = build_decoder(\n        decoder_input, embedding, encoder_states, config)\n    \n    # Output layer\n    output = layers.Dense(target_vocab_size, activation='softmax')(decoder_output)\n    \n    return models.Model([encoder_input, decoder_input], output)\n\ndef build_encoder(inputs, embedding, config):\n    \"\"\"Build encoder based on config\"\"\"\n    x = embedding(inputs)\n    rnn_type = config.rnn_type.lower()\n    \n    if rnn_type == 'lstm':\n        layer = layers.LSTM\n    elif rnn_type == 'gru':\n        layer = layers.GRU\n    else:\n        layer = layers.SimpleRNN\n    \n    for i in range(config.enc_layers):\n        return_sequences = (i < config.enc_layers - 1)\n        x = layer(\n            config.hidden_dim,\n            return_sequences=return_sequences,\n            return_state=True,\n            dropout=config.dropout,\n            name=f'enc_{rnn_type}_{i}'\n        )(x)\n    \n    return x if config.enc_layers == 1 else x[0], x[1:]\n\ndef build_decoder(inputs, embedding, initial_state, config):\n    \"\"\"Build decoder based on config\"\"\"\n    x = embedding(inputs)\n    rnn_type = config.rnn_type.lower()\n    \n    if rnn_type == 'lstm':\n        layer = layers.LSTM\n    elif rnn_type == 'gru':\n        layer = layers.GRU\n    else:\n        layer = layers.SimpleRNN\n    \n    for i in range(config.dec_layers):\n        x = layer(\n            config.hidden_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=config.dropout,\n            name=f'dec_{rnn_type}_{i}'\n        )(x, initial_state=initial_state)\n    \n    return x[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training with Weights & Biases\ndef train():\n    wandb.init()\n    config = wandb.config\n    \n    # Load data\n    loader = DataLoader(\"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/mr/lexicons\")\n    (train_enc, train_dec_in, train_dec_out), target_tokenizer = loader.preprocess_data(\n        \"mr.translit.sampled.train.tsv\", \n        \"mr.translit.sampled.dev.tsv\"\n    )\n    \n    # Build model\n    model = build_seq2seq_model(\n        len(loader.input_tokenizer.word_index) + 1,\n        len(target_tokenizer.word_index) + 1,\n        config\n    )\n    \n    model.compile(\n        optimizer=Adam(),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    model.fit(\n        [train_enc, train_dec_in],\n        train_dec_out,\n        batch_size=config.batch_size,\n        epochs=10,\n        validation_split=0.1,\n        callbacks=[WandbMetricsLogger(), WandbModelCheckpoint(\"models\")],\n        verbose=2\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure and Run Sweep\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'rnn_type': {'values': ['rnn', 'lstm', 'gru']},\n        'embedding_dim': {'values': [64, 128, 256]},\n        'hidden_dim': {'values': [128, 256, 512]},\n        'enc_layers': {'values': [1, 2]},\n        'dec_layers': {'values': [1, 2]},\n        'dropout': {'values': [0.1, 0.2, 0.3]},\n        'batch_size': {'values': [32, 64]},\n        'beam_width': {'values': [1, 3, 5]}\n    }\n}\n\nwandb.login(key=\"\") #add your api key here\nsweep_id = wandb.sweep(sweep_config, project=\"marathi-transliteration\")\nwandb.agent(sweep_id, function=train, count=15)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# from tensorflow.keras import layers, models, preprocessing\n# from tensorflow.keras.optimizers import Adam\n# import wandb\n# from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n\n# class TransliterationSystem:\n#     def __init__(self, data_dir):\n#         self.data_dir = data_dir\n#         self.input_tokenizer = None\n#         self.target_tokenizer = None\n        \n#     def _load_dataset(self, filename):\n#         \"\"\"Load TSV file into list of pairs\"\"\"\n#         with open(os.path.join(self.data_dir, filename), encoding='utf-8') as f:\n#             return [line.strip().split('\\t') for line in f if '\\t' in line]\n    \n#     def prepare_data(self):\n#         \"\"\"Load and preprocess training/validation data\"\"\"\n#         train_data = self._load_dataset(\"mr.translit.sampled.train.tsv\")\n#         val_data = self._load_dataset(\"mr.translit.sampled.dev.tsv\")\n        \n#         # Process text pairs\n#         train_source = [x[1] for x in train_data]\n#         train_target = [x[0] for x in train_data]\n        \n#         # Create tokenizers\n#         self.input_tokenizer = self._create_tokenizer(train_source + [x[1] for x in val_data])\n#         self.target_tokenizer = self._create_tokenizer(\n#             ['\\t' + t for t in train_target] + \n#             [t + '\\n' for t in train_target] +\n#             ['\\t' + t for t in [x[0] for x in val_data]] + \n#             [t + '\\n' for t in [x[0] for x in val_data]]\n#         )\n        \n#         # Prepare sequences\n#         max_source_len = max(len(s) for s in train_source)\n#         max_target_len = max(len(t) for t in train_target) + 1  # +1 for start/end tokens\n        \n#         train_enc = self._texts_to_padded_sequences(train_source, self.input_tokenizer, max_source_len)\n#         train_dec_in = self._texts_to_padded_sequences(['\\t' + t for t in train_target], self.target_tokenizer, max_target_len)\n#         train_dec_out = np.expand_dims(\n#             self._texts_to_padded_sequences([t + '\\n' for t in train_target], self.target_tokenizer, max_target_len),\n#             -1\n#         )\n        \n#         return (train_enc, train_dec_in, train_dec_out), self.input_tokenizer, self.target_tokenizer\n    \n#     def _create_tokenizer(self, texts):\n#         \"\"\"Create character-level tokenizer\"\"\"\n#         tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=False)\n#         tokenizer.fit_on_texts(texts)\n#         return tokenizer\n    \n#     def _texts_to_padded_sequences(self, texts, tokenizer, max_len):\n#         \"\"\"Convert texts to padded sequences\"\"\"\n#         seq = tokenizer.texts_to_sequences(texts)\n#         return preprocessing.sequence.pad_sequences(seq, padding='post', maxlen=max_len)\n\n# class Seq2SeqModelBuilder:\n#     \"\"\"Builds sequence-to-sequence models with different RNN types\"\"\"\n    \n#     RNN_TYPES = {\n#         'rnn': layers.SimpleRNN,\n#         'lstm': layers.LSTM,\n#         'gru': layers.GRU\n#     }\n    \n#     def __init__(self, input_vocab_size, target_vocab_size):\n#         self.input_vocab_size = input_vocab_size\n#         self.target_vocab_size = target_vocab_size\n    \n#     def build_model(self, rnn_type='lstm', embedding_dim=256, hidden_dim=512, \n#                    enc_layers=1, dec_layers=1, dropout=0.2):\n#         \"\"\"Build end-to-end seq2seq model\"\"\"\n        \n#         # Input layers\n#         encoder_input = layers.Input(shape=(None,))\n#         decoder_input = layers.Input(shape=(None,))\n        \n#         # Shared embedding\n#         embedding = layers.Embedding(self.input_vocab_size, embedding_dim, mask_zero=True)\n        \n#         # Encoder\n#         enc_output, enc_states = self._build_encoder(\n#             encoder_input, embedding, rnn_type, hidden_dim, enc_layers, dropout)\n        \n#         # Decoder\n#         decoder_output = self._build_decoder(\n#             decoder_input, embedding, rnn_type, hidden_dim, dec_layers, dropout, enc_states)\n        \n#         # Final output\n#         output = layers.Dense(self.target_vocab_size, activation='softmax')(decoder_output)\n        \n#         return models.Model([encoder_input, decoder_input], output)\n    \n#     def _build_encoder(self, inputs, embedding, rnn_type, hidden_dim, num_layers, dropout):\n#         \"\"\"Build encoder architecture\"\"\"\n#         x = embedding(inputs)\n#         rnn_class = self.RNN_TYPES[rnn_type.lower()]\n        \n#         for i in range(num_layers):\n#             return_sequences = (i < num_layers - 1)\n#             x = rnn_class(\n#                 hidden_dim,\n#                 return_sequences=return_sequences,\n#                 return_state=True,\n#                 dropout=dropout,\n#                 name=f'enc_{rnn_type}_{i}'\n#             )(x)\n        \n#         return x if num_layers == 1 else x[0], x[1:] if num_layers > 1 else x[1]\n    \n#     def _build_decoder(self, inputs, embedding, rnn_type, hidden_dim, num_layers, dropout, initial_state):\n#         \"\"\"Build decoder architecture\"\"\"\n#         x = embedding(inputs)\n#         rnn_class = self.RNN_TYPES[rnn_type.lower()]\n        \n#         for i in range(num_layers):\n#             x = rnn_class(\n#                 hidden_dim,\n#                 return_sequences=True,\n#                 return_state=True,\n#                 dropout=dropout,\n#                 name=f'dec_{rnn_type}_{i}'\n#             )(x, initial_state=initial_state)\n        \n#         return x[0]\n\n# class InferenceSystem:\n#     \"\"\"Handles model inference with beam search\"\"\"\n    \n#     def __init__(self, model, rnn_type, hidden_dim):\n#         self.rnn_type = rnn_type.lower()\n#         self._setup_inference_models(model, hidden_dim)\n        \n#     def _setup_inference_models(self, model, hidden_dim):\n#         \"\"\"Create encoder/decoder models for inference\"\"\"\n#         encoder_input = model.input[0]\n#         decoder_input = model.input[1]\n#         embedding = model.get_layer('embedding')\n        \n#         # Encoder model\n#         enc_output = embedding(encoder_input)\n#         rnn_layer = next(l for l in model.layers if l.name.startswith(f'enc_{self.rnn_type}'))\n        \n#         if self.rnn_type == 'lstm':\n#             _, state_h, state_c = rnn_layer(enc_output)\n#             self.encoder_model = models.Model(encoder_input, [state_h, state_c])\n#             self.state_size = 2\n#         elif self.rnn_type == 'gru':\n#             _, state_h = rnn_layer(enc_output)\n#             self.encoder_model = models.Model(encoder_input, [state_h])\n#             self.state_size = 1\n#         else:  # Simple RNN\n#             _, state_h = rnn_layer(enc_output)\n#             self.encoder_model = models.Model(encoder_input, [state_h])\n#             self.state_size = 1\n        \n#         # Decoder model\n#         decoder_states_input = [\n#             layers.Input(shape=(hidden_dim,)) for _ in range(self.state_size)\n#         ]\n#         decoder_emb = embedding(decoder_input)\n        \n#         decoder_rnn = next(l for l in model.layers if l.name.startswith(f'dec_{self.rnn_type}'))\n#         decoder_output = decoder_rnn(decoder_emb, initial_state=decoder_states_input)\n        \n#         dense_layer = model.get_layer('dense')\n#         decoder_output = dense_layer(decoder_output[0])\n        \n#         self.decoder_model = models.Model(\n#             [decoder_input] + decoder_states_input,\n#             [decoder_output] + list(decoder_output[1:])\n    \n#     def beam_search_decode(self, input_seq, tokenizer, beam_width=3, max_len=30):\n#         \"\"\"Decode sequence using beam search\"\"\"\n#         idx_to_char = {i: c for c, i in tokenizer.word_index.items()}\n#         idx_to_char[0] = ''\n        \n#         start_token = tokenizer.word_index['\\t']\n#         end_token = tokenizer.word_index['\\n']\n        \n#         states = self.encoder_model.predict(input_seq)\n#         if self.state_size == 1:\n#             states = [states]\n        \n#         beams = [([start_token], 0.0, states)]\n        \n#         for _ in range(max_len):\n#             candidates = []\n#             for seq, score, states in beams:\n#                 if seq[-1] == end_token:\n#                     candidates.append((seq, score, states))\n#                     continue\n                \n#                 target_seq = np.array([[seq[-1]]])\n#                 outputs = self.decoder_model.predict([target_seq] + states)\n#                 probs = outputs[0][0, -1, :]\n#                 top_tokens = np.argsort(probs)[-beam_width:]\n                \n#                 for token in top_tokens:\n#                     new_score = score - np.log(probs[token] + 1e-9)\n#                     candidate_seq = seq + [token]\n#                     candidates.append((candidate_seq, new_score, outputs[1:]))\n            \n#             beams = sorted(candidates, key=lambda x: x[1])[:beam_width]\n        \n#         best_seq = beams[0][0]\n#         return ''.join(idx_to_char.get(i, '') for i in best_seq[1:-1])\n\n# def run_sweep():\n#     \"\"\"Configure and run hyperparameter sweep\"\"\"\n#     sweep_config = {\n#         'method': 'bayes',\n#         'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n#         'parameters': {\n#             'rnn_type': {'values': ['rnn', 'lstm', 'gru']},\n#             'embedding_dim': {'values': [64, 128, 256]},\n#             'hidden_dim': {'values': [128, 256, 512]},\n#             'enc_layers': {'values': [1, 2]},\n#             'dec_layers': {'values': [1, 2]},\n#             'dropout': {'values': [0.1, 0.2, 0.3]},\n#             'batch_size': {'values': [32, 64]},\n#             'beam_width': {'values': [1, 3, 5]}\n#         }\n#     }\n    \n#     def sweep_train():\n#         with wandb.init() as run:\n#             config = run.config\n#             run.name = (f\"{config.rnn_type}_e{config.embedding_dim}_h{config.hidden_dim}_\"\n#                        f\"enc{config.enc_layers}_dec{config.dec_layers}_\"\n#                        f\"drop{config.dropout}_beam{config.beam_width}\")\n            \n#             # Initialize system\n#             system = TransliterationSystem(\"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/mr/lexicons\")\n#             (train_enc, train_dec_in, train_dec_out), _, target_tokenizer = system.prepare_data()\n            \n#             # Build model\n#             builder = Seq2SeqModelBuilder(\n#                 len(system.input_tokenizer.word_index) + 1,\n#                 len(target_tokenizer.word_index) + 1\n#             )\n#             model = builder.build_model(\n#                 rnn_type=config.rnn_type,\n#                 embedding_dim=config.embedding_dim,\n#                 hidden_dim=config.hidden_dim,\n#                 enc_layers=config.enc_layers,\n#                 dec_layers=config.dec_layers,\n#                 dropout=config.dropout\n#             )\n            \n#             model.compile(\n#                 optimizer=Adam(),\n#                 loss='sparse_categorical_crossentropy',\n#                 metrics=['accuracy']\n#             )\n            \n#             model.fit(\n#                 [train_enc, train_dec_in],\n#                 train_dec_out,\n#                 batch_size=config.batch_size,\n#                 epochs=10,\n#                 validation_split=0.1,\n#                 callbacks=[WandbMetricsLogger(), WandbModelCheckpoint(\"models\")],\n#                 verbose=2\n#             )\n    \n#     sweep_id = wandb.sweep(sweep_config, project=\"marathi-transliteration\")\n#     wandb.agent(sweep_id, function=sweep_train, count=15)\n\n# if __name__ == \"__main__\":\n#     wandb.login(key=\"fc2e1c64c097689579cbd81e98b023cd1e9d3ee3\")\n#     run_sweep()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Best Hyperparameters\nbest_config = {\n    'embedding_dim': 256,\n    'hidden_dim': 256,\n    'rnn_type': 'lstm',\n    'enc_layers': 1,\n    'dec_layers': 1,\n    'dropout': 0.3,\n    'batch_size': 64,\n    'epochs': 10\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Data Loader\ndata_loader = DataLoader(\"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/mr/lexicons\")\n(train_enc, train_dec_in, train_dec_out), target_tokenizer = data_loader.preprocess_data(\n    \"mr.translit.sampled.train.tsv\", \n    \"mr.translit.sampled.dev.tsv\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build and Train Best Model\nbest_model = Seq2SeqModelBuilder(\n    len(data_loader.input_tokenizer.word_index) + 1,\n    len(target_tokenizer.word_index) + 1\n).build_model(**best_config)\n\nbest_model.compile(\n    optimizer=Adam(),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start WandB run\nwandb.init(project=\"marathi-transliteration\", name=\"best_model_training\")\nbest_model.fit(\n    [train_enc, train_dec_in],\n    train_dec_out,\n    batch_size=best_config['batch_size'],\n    epochs=best_config['epochs'],\n    validation_split=0.1,\n    callbacks=[WandbMetricsLogger()],\n    verbose=2\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model\nbest_model.save(\"best_model.keras\")\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Test Data\ntest_pairs = data_loader.load_dataset(\"mr.translit.sampled.test.tsv\")\ntest_source = [x[1] for x in test_pairs]\ntest_target = [x[0] for x in test_pairs]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare Test Sequences\ntest_enc = data_loader._text_to_sequence(\n    test_source, \n    data_loader.input_tokenizer, \n    max_len=train_enc.shape[1]\n)\ntest_dec_in = data_loader._text_to_sequence(\n    ['\\t' + t for t in test_target],\n    target_tokenizer,\n    max_len=train_dec_in.shape[1]\n)\ntest_dec_out = np.expand_dims(\n    data_loader._text_to_sequence(\n        [t + '\\n' for t in test_target],\n        target_tokenizer,\n        max_len=train_dec_out.shape[1]\n    ),\n    -1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on Test Set\nwandb.init(project=\"marathi-transliteration\", name=\"best_model_testing\")\ntest_loss, test_acc = best_model.evaluate(\n    [test_enc, test_dec_in],\n    test_dec_out,\n    verbose=0\n)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nwandb.log({'test_accuracy': test_acc})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decoding Utilities\ndef decode_sequence(seq, tokenizer):\n    \"\"\"Convert sequence of indices to string\"\"\"\n    idx_to_char = {i: c for c, i in tokenizer.word_index.items()}\n    idx_to_char[0] = ''\n    decoded = []\n    for idx in seq:\n        if idx == 0:\n            continue\n        token = idx_to_char.get(idx, '')\n        if token == '\\n':\n            break\n        decoded.append(token)\n    return ''.join(decoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate Predictions\npreds = best_model.predict([test_enc, test_dec_in])\npred_indices = np.argmax(preds, axis=-1)\ndecoded_preds = [decode_sequence(seq, target_tokenizer) for seq in pred_indices]\ndecoded_refs = [t.replace('\\n', '') for t in test_target]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Predictions\nos.makedirs(\"predictions\", exist_ok=True)\nwith open(\"predictions/test_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n    for inp, pred, ref in zip(test_source, decoded_preds, decoded_refs):\n        f.write(f\"{inp}\\t{pred}\\t{ref}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Samples\nsample_indices = np.random.choice(len(test_source), 10, replace=False)\nprint(\"Prediction Samples:\")\nfor i, idx in enumerate(sample_indices):\n    print(f\"{i+1}. Input: {test_source[idx]}\")\n    print(f\"   Predicted: {decoded_preds[idx]}\")\n    print(f\"   Reference: {decoded_refs[idx]}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WandB Prediction Table\nwandb_table = wandb.Table(columns=[\"Input\", \"Prediction\", \"Reference\", \"Correct\"])\nfor idx in sample_indices:\n    correct = decoded_preds[idx] == decoded_refs[idx]\n    wandb_table.add_data(\n        test_source[idx],\n        decoded_preds[idx],\n        decoded_refs[idx],\n        \"✅\" if correct else \"❌\"\n    )\nwandb.log({\"predictions\": wandb_table})\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras import layers, models, preprocessing\n# from tensorflow.keras.optimizers import Adam\n# import wandb\n# from wandb.integration.keras import WandbMetricsLogger\n\n# class AttentionLayer(layers.Layer):\n#     \"\"\"Bahdanau Attention Layer with weight export capability\"\"\"\n#     def __init__(self, units, return_attention=False):\n#         super().__init__()\n#         self.W1 = layers.Dense(units)\n#         self.W2 = layers.Dense(units)\n#         self.V = layers.Dense(1)\n#         self.return_attention = return_attention\n#         self.units = units\n\n#     def call(self, query, values):\n#         # Add time axis for broadcasting\n#         query_with_time = tf.expand_dims(query, 2)  # (batch, dec_len, 1, units)\n#         values_with_time = tf.expand_dims(values, 1)  # (batch, 1, enc_len, units)\n        \n#         # Attention scores calculation\n#         score = self.V(tf.nn.tanh(\n#             self.W1(values_with_time) + self.W2(query_with_time)\n#         ))\n        \n#         attention_weights = tf.nn.softmax(score, axis=2)\n#         context_vector = tf.reduce_sum(attention_weights * values_with_time, axis=2)\n        \n#         if self.return_attention:\n#             return context_vector, tf.squeeze(attention_weights, -1)\n#         return context_vector\n\n# class AttentionTransliterator:\n#     def __init__(self, data_dir):\n#         self.data_dir = data_dir\n#         self.input_tokenizer = None\n#         self.target_tokenizer = None\n    \n#     def load_dataset(self, filename):\n#         \"\"\"Load and parse dataset file\"\"\"\n#         with open(os.path.join(self.data_dir, filename), 'r', encoding='utf-8') as f:\n#             return [line.strip().split('\\t') for line in f if '\\t' in line]\n    \n#     def preprocess_data(self):\n#         \"\"\"Load and prepare training/validation/test data\"\"\"\n#         train_pairs = self.load_dataset(\"mr.translit.sampled.train.tsv\")\n#         val_pairs = self.load_dataset(\"mr.translit.sampled.dev.tsv\")\n#         test_pairs = self.load_dataset(\"mr.translit.sampled.test.tsv\")\n        \n#         # Extract texts\n#         train_source = [x[1] for x in train_pairs]\n#         train_target = [x[0] for x in train_pairs]\n        \n#         # Create tokenizers\n#         self.input_tokenizer = self._create_tokenizer(train_source + [x[1] for x in val_pairs])\n#         self.target_tokenizer = self._create_tokenizer(\n#             ['\\t' + t for t in train_target] + \n#             [t + '\\n' for t in train_target]\n#         )\n        \n#         # Prepare sequences\n#         max_source_len = max(len(s) for s in train_source)\n#         max_target_len = max(len(t) for t in train_target) + 1  # +1 for start/end tokens\n        \n#         # Training data\n#         train_enc = self._text_to_sequence(train_source, self.input_tokenizer, max_source_len)\n#         train_dec_in = self._text_to_sequence(['\\t' + t for t in train_target], self.target_tokenizer, max_target_len)\n#         train_dec_out = np.expand_dims(\n#             self._text_to_sequence([t + '\\n' for t in train_target], self.target_tokenizer, max_target_len),\n#             -1\n#         )\n        \n#         # Validation data\n#         val_enc = self._text_to_sequence([x[1] for x in val_pairs], self.input_tokenizer, max_source_len)\n#         val_dec_in = self._text_to_sequence(['\\t' + x[0] for x in val_pairs], self.target_tokenizer, max_target_len)\n#         val_dec_out = np.expand_dims(\n#             self._text_to_sequence([x[0] + '\\n' for x in val_pairs], self.target_tokenizer, max_target_len),\n#             -1\n#         )\n        \n#         # Test data\n#         test_enc = self._text_to_sequence([x[1] for x in test_pairs], self.input_tokenizer, max_source_len)\n#         test_dec_in = self._text_to_sequence(['\\t' + x[0] for x in test_pairs], self.target_tokenizer, max_target_len)\n#         test_dec_out = np.expand_dims(\n#             self._text_to_sequence([x[0] + '\\n' for x in test_pairs], self.target_tokenizer, max_target_len),\n#             -1\n#         )\n        \n#         return (train_enc, train_dec_in, train_dec_out), \\\n#                (val_enc, val_dec_in, val_dec_out), \\\n#                (test_enc, test_dec_in, test_dec_out)\n    \n#     def _create_tokenizer(self, texts):\n#         \"\"\"Create character-level tokenizer\"\"\"\n#         tokenizer = preprocessing.text.Tokenizer(char_level=True, lower=False)\n#         tokenizer.fit_on_texts(texts)\n#         return tokenizer\n    \n#     def _text_to_sequence(self, texts, tokenizer, max_len):\n#         \"\"\"Convert texts to padded sequences\"\"\"\n#         seq = tokenizer.texts_to_sequences(texts)\n#         return preprocessing.sequence.pad_sequences(seq, padding='post', maxlen=max_len)\n\n# class AttentionModel:\n#     def __init__(self, input_vocab_size, target_vocab_size):\n#         self.input_vocab_size = input_vocab_size\n#         self.target_vocab_size = target_vocab_size\n    \n#     def build_model(self, embedding_dim=256, hidden_dim=512, dropout_rate=0.2):\n#         \"\"\"Build attention-based seq2seq model\"\"\"\n#         # Encoder\n#         encoder_inputs = layers.Input(shape=(None,))\n#         enc_emb = layers.Embedding(self.input_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n#         encoder_outputs, state_h, state_c = layers.LSTM(\n#             hidden_dim, return_sequences=True, return_state=True, dropout=dropout_rate\n#         )(enc_emb)\n        \n#         # Decoder\n#         decoder_inputs = layers.Input(shape=(None,))\n#         dec_emb = layers.Embedding(self.target_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n#         decoder_outputs = layers.LSTM(\n#             hidden_dim, return_sequences=True, return_state=True, dropout=dropout_rate\n#         )(dec_emb, initial_state=[state_h, state_c])[0]\n        \n#         # Attention\n#         context_vector, attention_weights = AttentionLayer(hidden_dim, return_attention=True)(\n#             decoder_outputs, encoder_outputs\n#         )\n#         concat_output = layers.Concatenate()([decoder_outputs, context_vector])\n        \n#         # Output\n#         outputs = layers.Dense(self.target_vocab_size, activation='softmax')(concat_output)\n        \n#         return models.Model([encoder_inputs, decoder_inputs], outputs)\n\n# def run_sweep():\n#     \"\"\"Configure and execute hyperparameter sweep\"\"\"\n#     sweep_config = {\n#         'method': 'bayes',\n#         'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n#         'parameters': {\n#             'embedding_dim': {'values': [128, 256]},\n#             'hidden_dim': {'values': [128, 256, 512]},\n#             'dropout_rate': {'values': [0.1, 0.2, 0.3]},\n#             'batch_size': {'values': [32, 64]},\n#             'learning_rate': {'min': 1e-4, 'max': 1e-3}\n#         }\n#     }\n    \n#     def train():\n#         wandb.init()\n#         config = wandb.config\n        \n#         # Initialize components\n#         transliterator = AttentionTransliterator(\n#             \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/mr/lexicons\"\n#         )\n#         (train_enc, train_dec_in, train_dec_out), \\\n#         (val_enc, val_dec_in, val_dec_out), _ = transliterator.preprocess_data()\n        \n#         # Build model\n#         model = AttentionModel(\n#             len(transliterator.input_tokenizer.word_index) + 1,\n#             len(transliterator.target_tokenizer.word_index) + 1\n#         ).build_model(\n#             embedding_dim=config.embedding_dim,\n#             hidden_dim=config.hidden_dim,\n#             dropout_rate=config.dropout_rate\n#         )\n        \n#         model.compile(\n#             optimizer=Adam(learning_rate=config.learning_rate),\n#             loss='sparse_categorical_crossentropy',\n#             metrics=['accuracy']\n#         )\n        \n#         # Train\n#         model.fit(\n#             [train_enc, train_dec_in],\n#             train_dec_out,\n#             validation_data=([val_enc, val_dec_in], val_dec_out),\n#             batch_size=config.batch_size,\n#             epochs=10,\n#             callbacks=[WandbMetricsLogger()],\n#             verbose=2\n#         )\n        \n#         # Save model if performance is good\n#         val_acc = model.history.history['val_accuracy'][-1]\n#         if val_acc > 0.85:  # Adjust threshold as needed\n#             model.save(f\"attention_model_{wandb.run.id}.keras\")\n    \n#     sweep_id = wandb.sweep(sweep_config, project=\"marathi-transliteration-attention\")\n#     wandb.agent(sweep_id, function=train, count=15)\n\n# def evaluate_best_model():\n#     \"\"\"Evaluate the best saved model on test set\"\"\"\n#     wandb.init(project=\"marathi-transliteration-attention\", name=\"best_model_evaluation\")\n    \n#     # Initialize components\n#     transliterator = AttentionTransliterator(\n#         \"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/mr/lexicons\"\n#     )\n#     _, _, (test_enc, test_dec_in, test_dec_out) = transliterator.preprocess_data()\n    \n#     # Load best model (replace with actual best run ID)\n#     best_model = models.load_model(\n#         \"attention_model_BEST_RUN_ID.keras\",\n#         custom_objects={'AttentionLayer': AttentionLayer}\n#     )\n    \n#     # Evaluate\n#     test_loss, test_acc = best_model.evaluate(\n#         [test_enc, test_dec_in],\n#         test_dec_out,\n#         verbose=1\n#     )\n#     print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n    \n#     # Log results\n#     wandb.log({\n#         'test_accuracy': test_acc,\n#         'test_loss': test_loss\n#     })\n    \n#     # Generate predictions\n#     preds = best_model.predict([test_enc, test_dec_in])\n#     pred_indices = np.argmax(preds, axis=-1)\n    \n#     # Save predictions\n#     os.makedirs(\"predictions\", exist_ok=True)\n#     with open(\"predictions/attention_predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n#         for i in range(len(test_enc)):\n#             input_text = transliterator.input_tokenizer.sequences_to_texts([test_enc[i]])[0]\n#             pred_text = transliterator.target_tokenizer.sequences_to_texts([pred_indices[i]])[0]\n#             true_text = transliterator.target_tokenizer.sequences_to_texts([test_dec_out[i]])[0]\n#             f.write(f\"{input_text}\\t{pred_text}\\t{true_text}\\n\")\n    \n#     wandb.finish()\n\n# if __name__ == \"__main__\":\n#     wandb.login(key=\"your_api_key_here\")  # Replace with your WandB key\n    \n#     # Run either the sweep or evaluation\n#     run_sweep()  # Comment this out after sweep is done\n#     # evaluate_best_model()  # Uncomment after selecting best model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttentionLayer(tf.keras.layers.Layer):\n    def __init__(self, units, return_attention=False):\n        super().__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n        self.return_attention = return_attention\n\n    def call(self, query, values):\n        # Add time axis for broadcasting\n        query = tf.expand_dims(query, 2)  # (batch, dec_len, 1, units)\n        values = tf.expand_dims(values, 1)  # (batch, 1, enc_len, units)\n        \n        # Attention calculation\n        score = self.V(tf.nn.tanh(\n            self.W1(values) + self.W2(query)\n        ))\n        attention_weights = tf.nn.softmax(score, axis=2)\n        context = tf.reduce_sum(attention_weights * values, axis=2)\n        \n        if self.return_attention:\n            return context, tf.squeeze(attention_weights, -1)\n        return context\n\ndef build_attention_model(vocab_size_input, vocab_size_target):\n    # Encoder\n    encoder_input = tf.keras.Input(shape=(None,))\n    enc_emb = tf.keras.layers.Embedding(vocab_size_input, 256, mask_zero=True)(encoder_input)\n    encoder_output, state_h, state_c = tf.keras.layers.LSTM(\n        256, return_sequences=True, return_state=True\n    )(enc_emb)\n    \n    # Decoder\n    decoder_input = tf.keras.Input(shape=(None,))\n    dec_emb = tf.keras.layers.Embedding(vocab_size_target, 256, mask_zero=True)(decoder_input)\n    decoder_output = tf.keras.layers.LSTM(\n        256, return_sequences=True, return_state=True\n    )(dec_emb, initial_state=[state_h, state_c])[0]\n    \n    # Attention\n    context, attention_weights = AttentionLayer(256, return_attention=True)(\n        decoder_output, encoder_output\n    )\n    concat = tf.keras.layers.Concatenate()([decoder_output, context])\n    \n    # Output\n    outputs = tf.keras.layers.Dense(vocab_size_target, activation='softmax')(concat)\n    \n    return tf.keras.Model([encoder_input, decoder_input], outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize data\nattention_data = TransliterationSystem(\"/kaggle/input/dakshinadataset/dakshina_dataset_v1.0/mr/lexicons\")\n(train_enc, train_dec_in, train_dec_out), target_tokenizer = attention_data.prepare_data()\n\n# Build attention model\nattention_model = build_attention_model(\n    len(attention_data.input_tokenizer.word_index) + 1,\n    len(target_tokenizer.word_index) + 1\n)\n\nattention_model.compile(\n    optimizer=Adam(),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train with WandB\nwandb.init(project=\"marathi-transliteration\", name=\"attention_model\")\nattention_model.fit(\n    [train_enc, train_dec_in],\n    train_dec_out,\n    batch_size=64,\n    epochs=10,\n    validation_split=0.1,\n    callbacks=[WandbMetricsLogger()],\n    verbose=2\n)\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_attention(model, input_seq, tokenizer):\n    # Create inference model\n    encoder_input = model.input[0]\n    encoder_output = model.layers[4].output  # Encoder LSTM output\n    decoder_input = model.input[1]\n    decoder_lstm = model.layers[5]\n    attention_layer = model.layers[7]\n    \n    # Encoder inference model\n    encoder_model = tf.keras.Model(encoder_input, encoder_output)\n    \n    # Decoder inference model\n    decoder_state_input_h = tf.keras.Input(shape=(256,))\n    decoder_state_input_c = tf.keras.Input(shape=(256,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(\n        model.layers[3](decoder_input), initial_state=decoder_states_inputs\n    )\n    \n    context, attention = attention_layer(decoder_outputs, encoder_output)\n    decoder_model = tf.keras.Model(\n        [decoder_input] + decoder_states_inputs,\n        [model.layers[8](tf.keras.layers.Concatenate()([decoder_outputs, context]))] + \n        [state_h, state_c, attention]\n    )\n    \n    # Run inference\n    enc_output = encoder_model.predict(input_seq)\n    states = [np.zeros((1, 256)), np.zeros((1, 256))]\n    attention_weights = []\n    \n    target_seq = np.array([[tokenizer.word_index['\\t']]])\n    for _ in range(30):  # Max output length\n        outputs = decoder_model.predict([target_seq] + states)\n        output_token, h, c, attention = outputs[0], outputs[1], outputs[2], outputs[3]\n        attention_weights.append(attention[0])\n        \n        sampled_token = np.argmax(output_token[0, -1, :])\n        if sampled_token == tokenizer.word_index['\\n']:\n            break\n            \n        target_seq = np.array([[sampled_token]])\n        states = [h, c]\n    \n    return np.array(attention_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example visualization\nsample_idx = 0\ninput_seq = train_enc[sample_idx:sample_idx+1]\nattention_weights = visualize_attention(attention_model, input_seq, target_tokenizer)\n\n# Plot attention\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 8))\nplt.imshow(attention_weights.T, cmap='viridis')\nplt.xlabel('Decoder Step')\nplt.ylabel('Encoder Step')\nplt.title('Attention Weights Visualization')\nplt.colorbar()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sweep Config.\n\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'embedding_dim': {\n            'values': [128, 256]\n        },\n        'hidden_dim': {\n            'values': [128, 256, 512]\n        },\n        'dropout_rate': {\n            'values': [0.0, 0.2, 0.3]\n        },\n        'batch_size': {\n            'values': [32, 64]\n        },\n        'learning_rate': {\n            'min': 1e-4,\n            'max': 1e-3\n        }\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Training function\n\ndef attention_sweep_train():\n    wandb.init()\n    config = wandb.config\n    \n    # Build model with sweep parameters\n    model = build_attention_model(\n        vocab_size_input=len(data_loader.input_tokenizer.word_index) + 1,\n        vocab_size_target=len(target_tokenizer.word_index) + 1,\n        embedding_dim=config.embedding_dim,\n        hidden_dim=config.hidden_dim,\n        dropout_rate=config.dropout_rate\n    )\n    \n    model.compile(\n        optimizer=Adam(learning_rate=config.learning_rate),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    history = model.fit(\n        [train_enc, train_dec_in],\n        train_dec_out,\n        validation_data=([val_enc, val_dec_in], val_dec_out),\n        batch_size=config.batch_size,\n        epochs=10,\n        callbacks=[WandbMetricsLogger()],\n        verbose=2\n    )\n    \n    # Save the best model\n    if history.history['val_accuracy'][-1] > 0.85: \n        model.save(f\"attention_model_{wandb.run.id}.keras\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize sweep\nwandb.login(key=\"your_api_key_here\")  # Replace with your actual key\nsweep_id = wandb.sweep(sweep_config, project=\"marathi-transliteration-attention\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start sweep agent\nwandb.agent(sweep_id, function=attention_sweep_train, count=15)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After sweep completes, manually load best model from WandB\nbest_model = tf.keras.models.load_model(\"attention_model_[BEST_RUN_ID].keras\", \n                                      custom_objects={'AttentionLayer': AttentionLayer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on test set\ntest_loss, test_acc = best_model.evaluate(\n    [test_enc, test_dec_in],\n    test_dec_out,\n    verbose=1\n)\nprint(f\"\\nBest Model Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Log to WandB\nwandb.init(project=\"marathi-transliteration-attention\", name=\"best_model_eval\")\nwandb.log({\n    'test_accuracy': test_acc,\n    'test_loss': test_loss\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_attention_weights(model, input_seq, output_seq, input_tokenizer, target_tokenizer):\n    # Create inference models\n    encoder_inputs = model.input[0]\n    encoder_outputs = model.layers[4].output  # Encoder LSTM\n    decoder_lstm = model.layers[5]\n    attention_layer = model.layers[7]\n    \n    # Encoder inference model\n    encoder_model = tf.keras.Model(encoder_inputs, encoder_outputs)\n    \n    # Decoder inference model\n    decoder_state_input_h = tf.keras.Input(shape=(config.hidden_dim,))\n    decoder_state_input_c = tf.keras.Input(shape=(config.hidden_dim,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(\n        model.layers[3](model.input[1]), \n        initial_state=decoder_states_inputs\n    )\n    \n    context_vector, attention_weights = attention_layer(decoder_outputs, encoder_outputs)\n    decoder_model = tf.keras.Model(\n        [model.input[1]] + decoder_states_inputs,\n        [model.layers[8](tf.keras.layers.Concatenate()([decoder_outputs, context_vector]))] +\n        [state_h, state_c, attention_weights]\n    )\n    \n    # Run inference\n    attention_plot = np.zeros((output_seq.shape[1], input_seq.shape[1]))\n    states = encoder_model.predict(input_seq)\n    dec_states = [states[1], states[2]]  # Initial states\n    \n    for t in range(output_seq.shape[1]):\n        outputs = decoder_model.predict([output_seq[:, t:t+1]] + dec_states)\n        attention_weights = outputs[3][0, 0, :]\n        attention_plot[t] = attention_weights\n        \n    # Plotting\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attention_plot[:len(output_seq[0]), :len(input_seq[0])], cmap='viridis')\n    \n    # Axis labels\n    input_tokens = [input_tokenizer.index_word.get(i, '') for i in input_seq[0]]\n    output_tokens = [target_tokenizer.index_word.get(i, '') for i in output_seq[0]]\n    \n    ax.set_xticks(range(len(input_tokens)))\n    ax.set_yticks(range(len(output_tokens)))\n    ax.set_xticklabels(input_tokens, rotation=90)\n    ax.set_yticklabels(output_tokens)\n    \n    plt.colorbar(cax)\n    plt.title(\"Attention Weights Heatmap\")\n    plt.show()\n    return fig\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example visualization\nsample_idx = 0  # Change to visualize different samples\nplot_attention_weights(\n    best_model,\n    test_enc[sample_idx:sample_idx+1],\n    test_dec_in[sample_idx:sample_idx+1],\n    data_loader.input_tokenizer,\n    target_tokenizer\n)\n\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained attention model\nattention_model = load_model('best_attention_model.keras', \n                           custom_objects={'BahdanauAttention': BahdanauAttention})\n\n# Initialize WandB\nwandb.init(project=\"Assignment_03\", name='Attention_Final_Evaluation')\n\n# Evaluate on test set\ntest_loss, test_acc = attention_model.evaluate(\n    [test_encoder_input, test_decoder_input], \n    test_target_output,\n    verbose=2\n)\nprint(f\"Test Accuracy (Attention Model): {test_acc:.4f}\")\nwandb.log({'test_accuracy': test_acc})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions\nos.makedirs(\"predictions_attention\", exist_ok=True)\nattention_preds = attention_model.predict([test_encoder_input, test_decoder_input])\nattention_pred_indices = np.argmax(attention_preds, axis=-1)\n\n# Decoding utilities\ndef decode_sequence(seq):\n    decoded = []\n    for idx in seq:\n        if idx == 0:\n            continue\n        token = index_to_char.get(idx, '')\n        if token == '\\n':\n            break\n        decoded.append(token)\n    return ''.join(decoded)\n\ndecoded_attention_preds = [decode_sequence(seq) for seq in attention_pred_indices]\ndecoded_refs = [t.replace('\\n', '') for t in test_deva_out]\n\n# Save predictions\nwith open(\"predictions_attention/test_predictions.txt\", \"w\", encoding='utf-8') as f:\n    for inp, pred, ref in zip(test_lat, decoded_attention_preds, decoded_refs):\n        f.write(f\"{inp}\\t{pred}\\t{ref}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load vanilla model predictions\nvanilla_preds = []\nwith open(\"predictions_vanilla/test_predictions.txt\", encoding=\"utf-8\") as f:\n    for line in f:\n        parts = line.strip().split('\\t')\n        if len(parts) >= 2:\n            vanilla_preds.append(parts[1])  # Predicted text is second column","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find improvements\nprint(\"Attention Model Improvements:\")\nimprovement_count = 0\nfor i, (v_pred, a_pred, ref) in enumerate(zip(vanilla_preds, decoded_attention_preds, decoded_refs)):\n    if v_pred != ref and a_pred == ref:\n        print(f\"Case {improvement_count+1}:\")\n        print(f\"  Input: {test_lat[i]}\")\n        print(f\"  Vanilla: {v_pred}\")\n        print(f\"  Attention: {a_pred}\")\n        print(f\"  Reference: {ref}\\n\")\n        improvement_count += 1\n    if improvement_count >= 5:  # Show top 5 improvements\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find \"jevan\" in test set\ntarget_word = \"ikbala\"\nfound_idx = -1\nfor i, word in enumerate(test_lat):\n    if word == target_word:\n        found_idx = i\n        break\n\nif found_idx == -1:\n    print(f\"❌ '{target_word}' not found in test set\")\nelse:\n    print(f\"✅ Found '{target_word}' at index {found_idx}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build inference models\nencoder_inputs = attention_model.input[0]\nencoder_outputs = attention_model.layers[4].output  # Encoder LSTM\nencoder_model = Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = attention_model.input[1]\ndecoder_lstm = attention_model.layers[5]\nattention_layer = attention_model.layers[7]\n\n# Decoder inference model\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs = decoder_lstm(\n    attention_model.layers[3](decoder_inputs),\n    initial_state=decoder_states_inputs\n)[0]\n\ncontext_vector, attention_weights = attention_layer(\n    decoder_outputs, encoder_outputs\n)\n\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [attention_model.layers[8](context_vector), attention_weights]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decode with attention tracking\ndef decode_with_attention(input_seq, max_len=30):\n    # Encode input\n    encoder_out = encoder_model.predict(input_seq)\n    \n    # Initialize decoder\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n    \n    decoded = []\n    attention_weights = []\n    \n    # Initialize states\n    states = [np.zeros((1, 256)), np.zeros((1, 256))]\n    \n    for _ in range(max_len):\n        # Get output and attention\n        output_tokens, attn = decoder_model.predict([target_seq] + states)\n        \n        # Sample token\n        sampled_token = np.argmax(output_tokens[0, -1, :])\n        decoded.append(sampled_token)\n        attention_weights.append(attn[0][0])  # Get weights for this step\n        \n        # Exit condition\n        if sampled_token == target_tokenizer.word_index['\\n']:\n            break\n            \n        # Update states and target sequence\n        states = [output_tokens[1], output_tokens[2]]\n        target_seq[0, 0] = sampled_token\n    \n    # Convert to text\n    decoded_text = ''.join([index_to_char.get(idx, '') for idx in decoded])\n    \n    return decoded_text, np.array(attention_weights)\n\n# Process \"ikbala\"\ninput_seq = test_encoder_input[found_idx:found_idx+1]\ndecoded_text, attention_weights = decode_with_attention(input_seq)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_attention(input_text, output_text, attention_weights):\n    plt.figure(figsize=(8, 6))\n    ax = sns.heatmap(\n        attention_weights,\n        xticklabels=list(input_text),\n        yticklabels=list(output_text),\n        cmap=\"YlOrRd\",\n        linewidths=0.5,\n        annot=True,\n        fmt=\".2f\",\n        cbar=False\n    )\n    plt.title(f\"Attention Weights: '{input_text}' → '{output_text}'\")\n    plt.xlabel(\"Input Characters\")\n    plt.ylabel(\"Output Characters\")\n    plt.tight_layout()\n    plt.show()\n\nprint(f\"\\nAttention Visualization for 'jevan':\")\nprint(f\"  Input: {test_lat[found_idx]}\")\nprint(f\"  Prediction: {decoded_text}\")\nprint(f\"  Reference: {decoded_refs[found_idx]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_attention(\n    input_text=test_lat[found_idx],\n    output_text=decoded_text,\n    attention_weights=attention_weights\n)\n\n# Save visualization\nos.makedirs(\"attention_visualizations\", exist_ok=True)\nplt.savefig(f\"attention_visualizations/jevan_attention.png\")\nplt.close()\n\n\n# Create WandB table\nwandb_table = wandb.Table(columns=[\"Input\", \"Prediction\", \"Reference\", \"Correct\"])\n\n# Add sample predictions\nsample_indices = np.random.choice(len(test_lat), 10, replace=False)\nfor idx in sample_indices:\n    input_seq = test_encoder_input[idx:idx+1]\n    pred_text, _ = decode_with_attention(input_seq)\n    correct = pred_text == decoded_refs[idx]\n    wandb_table.add_data(\n        test_lat[idx],\n        pred_text,\n        decoded_refs[idx],\n        \"✅\" if correct else \"❌\"\n    )\n\n# Special row for \"jevan\"\nwandb_table.add_data(\n    test_lat[found_idx],\n    decoded_text,\n    decoded_refs[found_idx],\n    \"✅\" if decoded_text == decoded_refs[found_idx] else \"❌\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Log attention heatmap\nattention_img = wandb.Image(f\"attention_visualizations/jevan_attention.png\")\nwandb.log({\n    \"predictions\": wandb_table,\n    \"attention_heatmap\": attention_img,\n    \"test_accuracy\": test_acc\n})\n\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}